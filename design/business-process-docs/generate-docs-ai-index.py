#!/usr/bin/env python3
"""
ç”Ÿæˆ docs-ai æ–‡æª”ç´¢å¼•å’Œå¼•ç”¨çµ±è¨ˆï¼ˆå¼·å¼•ç”¨èˆ‡å¼±å¼•ç”¨ç‰ˆæœ¬ï¼‰
- å¼·å¼•ç”¨ï¼š@docs-ai/ é–‹é ­çš„å¼•ç”¨ï¼ˆæ¬Šé‡ 1.0ï¼‰
- å¼±å¼•ç”¨ï¼šdocs-ai/ é–‹é ­ä½†ç„¡ @ çš„å¼•ç”¨ï¼ˆæ¬Šé‡ 0.3ï¼‰
- å…§éƒ¨å¼•ç”¨ï¼šæ–‡æª”ä¹‹é–“çš„äº’ç›¸å¼•ç”¨é—œä¿‚

ä½œè€…ï¼šAI Agent
æ›´æ–°æ—¥æœŸï¼š2025-09-19
"""

import json
import os
import re
from pathlib import Path
from collections import defaultdict
from datetime import datetime

# åŸºç¤è·¯å¾‘é…ç½®
ROSAGV_ROOT = Path("/home/ct/RosAGV")
DOCS_AI_PATH = ROSAGV_ROOT / "docs-ai"
OUTPUT_PATH = ROSAGV_ROOT / "design/business-process-docs/js/docs-ai-index.json"

# å¼•ç”¨æ¬Šé‡é…ç½®
STRONG_WEIGHT = 1.0  # å¼·å¼•ç”¨æ¬Šé‡
WEAK_WEIGHT = 0.3    # å¼±å¼•ç”¨æ¬Šé‡

# æ–‡æª”åˆ†é¡å®šç¾©
CATEGORIES = {
    "core": {
        "name": "ğŸ”¥ æ ¸å¿ƒåŸå‰‡",
        "description": "AI Agent å¿…è®€æ–‡æª”",
        "patterns": [
            "operations/development/core/",
            "operations/tools/unified-tools.md"
        ]
    },
    "architecture": {
        "name": "ğŸ“š ç³»çµ±æ¶æ§‹",
        "description": "ç³»çµ±è¨­è¨ˆå’Œæ¶æ§‹",
        "patterns": [
            "context/system/",
            "context/workspaces/"
        ]
    },
    "testing": {
        "name": "ğŸ§ª æ¸¬è©¦æ¨™æº–",
        "description": "æ¸¬è©¦ç›¸é—œæ–‡æª”",
        "patterns": [
            "operations/development/testing/"
        ]
    },
    "operations": {
        "name": "ğŸ“– æ“ä½œæŒ‡å—",
        "description": "é–‹ç™¼å’Œé‹ç¶­æŒ‡å°",
        "patterns": [
            "operations/guides/",
            "operations/deployment/",
            "operations/development/"
        ]
    },
    "knowledge": {
        "name": "ğŸ·ï¸ é ˜åŸŸçŸ¥è­˜",
        "description": "æ¥­å‹™å’ŒæŠ€è¡“çŸ¥è­˜",
        "patterns": [
            "knowledge/"
        ]
    }
}

# ä¸‰å±¤æ¶æ§‹å®šç¾©
LAYER_DEFINITIONS = {
    "layer1": {
        "name": "ğŸŒ é€šç”¨å±¤",
        "description": "ç³»çµ±æ¶æ§‹ã€æ ¸å¿ƒåŸå‰‡ã€é€šç”¨å·¥å…·",
        "target": "æ ¹ç›®éŒ„ CLAUDE.md",
        "patterns": [
            # ç³»çµ±æ¶æ§‹ï¼ˆå¿…é ˆç†è§£ï¼‰
            "context/system/dual-environment.md",
            "context/system/rosagv-overview.md",
            "context/system/technology-stack.md",
            "context/system/language-configuration.md",
            # æ ¸å¿ƒé–‹ç™¼åŸå‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰
            "operations/development/core/core-principles.md",
            "operations/development/core/linus-torvalds-ai-agent-principles.md",
            "operations/development/core/documentation-standards.md",
            # é€šç”¨å·¥å…·èˆ‡æ“ä½œï¼ˆæ—¥å¸¸ä½¿ç”¨ï¼‰
            "operations/tools/unified-tools.md",
            "operations/development/docker-development.md",
            "operations/guides/system-diagnostics.md",
            "operations/guides/troubleshooting.md"
        ]
    },
    "layer2": {
        "name": "ğŸ”§ å·¥ä½œç©ºé–“å±¤",
        "description": "é ˜åŸŸçŸ¥è­˜ã€é–‹ç™¼æµç¨‹ã€é€šç”¨å”è­°",
        "target": "*_ws/CLAUDE.md",
        "patterns": [
            # å·¥ä½œç©ºé–“æ¶æ§‹
            "context/workspaces/",
            # é€šç”¨å”è­°å’Œä»‹é¢
            "knowledge/protocols/ros2-interfaces.md",
            "knowledge/protocols/zenoh-rmw.md",
            # é–‹ç™¼æµç¨‹
            "operations/development/ros2/ros2-development.md",
            "operations/development/testing/testing-standards.md",
            "operations/development/database-operations.md",
            # é ˜åŸŸçŸ¥è­˜ï¼ˆæ ¹æ“šå·¥ä½œç©ºé–“é¸æ“‡æ€§å¼•ç”¨ï¼‰
            "knowledge/agv-domain/",
            "knowledge/system/",
            "knowledge/protocols/",
            "knowledge/business/"
        ]
    },
    "layer3": {
        "name": "ğŸ”¬ å°ˆæ¥­å±¤",
        "description": "ç‰¹å®šå¯¦ä½œã€å°ˆæ¥­ç´°ç¯€ã€æ¨¡çµ„ç‰¹å®š",
        "target": "src/*/CLAUDE.md",
        "patterns": [
            # é«˜åº¦å°ˆæ¥­åŒ–æ–‡æª”
            "knowledge/agv-domain/robot-pgno-rules.md",
            "knowledge/agv-domain/magic-value-analysis.md",
            "knowledge/agv-domain/write-path-state-analysis.md",
            "knowledge/system/tafl/",
            "operations/development/testing/ros2-pytest-testing.md"
        ]
    }
}

def determine_document_layer(doc_path):
    """æ ¹æ“šæ–‡æª”è·¯å¾‘åˆ¤æ–·å…¶æ‡‰å±¬æ–¼å“ªå€‹å±¤ç´š"""
    # ç²¾ç¢ºåŒ¹é…å„ªå…ˆ
    for layer_key, layer_info in LAYER_DEFINITIONS.items():
        for pattern in layer_info["patterns"]:
            if pattern.endswith(".md"):
                # ç²¾ç¢ºæª”æ¡ˆåŒ¹é…
                if doc_path == pattern:
                    return layer_key
            elif pattern.endswith("/"):
                # ç›®éŒ„åŒ¹é…
                if doc_path.startswith(pattern):
                    return layer_key

    # åŸºæ–¼å…§å®¹å’Œè·¯å¾‘çš„é€šç”¨è¦å‰‡
    # Layer 1: é€šç”¨ç³»çµ±ç´šçŸ¥è­˜
    if any(p in doc_path for p in [
        "context/system/",
        "operations/development/core/",
        "operations/tools/unified-tools.md",
        "operations/guides/system-diagnostics.md",
        "operations/guides/troubleshooting.md"
    ]):
        return "layer1"

    # Layer 3: é«˜åº¦å°ˆæ¥­åŒ–
    if any(p in doc_path for p in [
        "robot-pgno-rules",
        "magic-value-analysis",
        "write-path-state-analysis",
        "/tafl/tafl-",
        "ros2-pytest-testing"
    ]):
        return "layer3"

    # Layer 2: å·¥ä½œç©ºé–“ç´šï¼ˆé»˜èªï¼‰
    return "layer2"

def scan_docs_ai_files():
    """æƒææ‰€æœ‰ docs-ai ç›®éŒ„ä¸‹çš„ .md æ–‡ä»¶"""
    docs = {}

    for md_file in DOCS_AI_PATH.rglob("*.md"):
        relative_path = md_file.relative_to(DOCS_AI_PATH)
        path_str = str(relative_path).replace('\\', '/')

        # è®€å–æ–‡ä»¶æ¨™é¡Œå’Œæè¿°
        title = path_str
        description = ""

        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines[:10]:  # åªæª¢æŸ¥å‰10è¡Œ
                    if line.startswith("# "):
                        title = line[2:].strip()
                    elif line.startswith("## ğŸ¯ é©ç”¨å ´æ™¯"):
                        # è®€å–ä¸‹ä¸€è¡Œä½œç‚ºæè¿°
                        idx = lines.index(line)
                        if idx + 1 < len(lines):
                            description = lines[idx + 1].strip().lstrip("- ")
                        break
        except:
            pass

        # åˆ¤æ–·åˆ†é¡
        category = "other"
        for cat_key, cat_info in CATEGORIES.items():
            for pattern in cat_info["patterns"]:
                if pattern in path_str:
                    category = cat_key
                    break
            if category != "other":
                break

        # åˆ¤æ–·å±¤ç´š
        layer = determine_document_layer(path_str)

        docs[path_str] = {
            "path": path_str,
            "title": title,
            "description": description[:100] if description else "",
            "category": category,
            "layer": layer,  # æ–°å¢ï¼šæ–‡æª”å±¤ç´š

            # å¼•ç”¨çµ±è¨ˆï¼ˆå¼·å¼•ç”¨èˆ‡å¼±å¼•ç”¨ï¼‰
            "strong_references": 0,      # å¼·å¼•ç”¨æ¬¡æ•¸
            "weak_references": 0,         # å¼±å¼•ç”¨æ¬¡æ•¸
            "total_weighted_refs": 0,     # åŠ æ¬Šç¸½åˆ†

            # å‘å¾Œå…¼å®¹æ¬„ä½
            "references": 0,              # ç¸½å¼•ç”¨æ¬¡æ•¸ï¼ˆå¼·+å¼±ï¼‰

            # å¼•ç”¨ä¾†æº
            "strong_referenced_by": [],  # å¼·å¼•ç”¨ä¾†æº
            "weak_referenced_by": [],    # å¼±å¼•ç”¨ä¾†æº
            "referenced_by": [],         # æ‰€æœ‰å¼•ç”¨ä¾†æºï¼ˆå‘å¾Œå…¼å®¹ï¼‰

            # å±¤ç´šå¼•ç”¨ä¾†æºï¼ˆæ–°å¢ï¼‰
            "layer_references": {
                "layer1": [],  # è¢«å“ªäº›æ ¹å±¤ç´š CLAUDE.md å¼•ç”¨
                "layer2": [],  # è¢«å“ªäº›å·¥ä½œç©ºé–“å±¤ç´š CLAUDE.md å¼•ç”¨
                "layer3": []   # è¢«å“ªäº›å°ˆæ¥­å±¤ç´š CLAUDE.md å¼•ç”¨
            },

            # å‘å¤–å¼•ç”¨
            "references_to": [],          # è©²æ–‡æª”å¼•ç”¨çš„å…¶ä»–æ–‡æª”

            "size": md_file.stat().st_size,
            "modified": md_file.stat().st_mtime
        }

    return docs

def determine_claude_file_layer(claude_file_path):
    """æ ¹æ“š CLAUDE.md æª”æ¡ˆè·¯å¾‘åˆ¤æ–·å…¶å±¤ç´š"""
    path_str = str(claude_file_path)

    # Layer 1: æ ¹ç›®éŒ„ CLAUDE.md
    if path_str == "CLAUDE.md":
        return "layer1"

    # Layer 2: å·¥ä½œç©ºé–“å±¤ç´š (*_ws/CLAUDE.md)
    if "_ws/CLAUDE.md" in path_str and "/src/" not in path_str:
        return "layer2"

    # Layer 3: å°ˆæ¥­æ¨¡çµ„å±¤ç´š (src/*/CLAUDE.md)
    if "/src/" in path_str and "CLAUDE.md" in path_str:
        return "layer3"

    # å…¶ä»–ï¼ˆå¦‚ README.md, .github/*.mdï¼‰
    return "other"

def scan_claude_references():
    """æƒææ‰€æœ‰ CLAUDE.md æ–‡ä»¶ä¸­çš„ @docs-ai å¼•ç”¨ï¼ˆå€åˆ†å¼·å¼•ç”¨å’Œå¼±å¼•ç”¨ï¼‰"""
    strong_references = defaultdict(list)
    weak_references = defaultdict(list)
    layer_references = defaultdict(lambda: defaultdict(list))  # æ–°å¢ï¼šå±¤ç´šå¼•ç”¨è¿½è¹¤

    # æ”¶é›†æ‰€æœ‰éœ€è¦æƒæçš„æª”æ¡ˆ
    scan_files = []

    # æƒææ ¹ç›®éŒ„çš„ CLAUDE.md å’Œ README.md
    scan_files.extend(list(ROSAGV_ROOT.glob("CLAUDE.md")))
    scan_files.extend(list(ROSAGV_ROOT.glob("README.md")))
    # æƒææ‰€æœ‰å·¥ä½œç©ºé–“çš„ CLAUDE.md
    scan_files.extend(ROSAGV_ROOT.glob("app/**/CLAUDE.md"))
    # æƒæ .github ä¸­çš„æŒ‡å°æ–‡ä»¶
    scan_files.extend(ROSAGV_ROOT.glob(".github/*.md"))

    for scan_file in scan_files:
        relative_file = scan_file.relative_to(ROSAGV_ROOT)
        file_layer = determine_claude_file_layer(relative_file)

        try:
            with open(scan_file, 'r', encoding='utf-8') as f:
                content = f.read()

                # å¼·å¼•ç”¨ï¼š@docs-ai/xxx
                strong_pattern = r'@docs-ai/([^\s\]]+\.md)'
                strong_matches = re.findall(strong_pattern, content)

                for match in strong_matches:
                    strong_references[match].append(str(relative_file))
                    # è¨˜éŒ„å±¤ç´šå¼•ç”¨
                    if file_layer in ["layer1", "layer2", "layer3"]:
                        layer_references[match][file_layer].append(str(relative_file))

                # å¼±å¼•ç”¨ï¼šdocs-ai/xxx ï¼ˆä¸åŒ…å« @ ç¬¦è™Ÿï¼‰
                # ä½¿ç”¨è² å‘å¾Œé¡§æ–·è¨€ç¢ºä¿å‰é¢ä¸æ˜¯ @
                weak_pattern = r'(?<!@)docs-ai/([^\s\]]+\.md)'
                weak_matches = re.findall(weak_pattern, content)

                for match in weak_matches:
                    weak_references[match].append(str(relative_file))

        except Exception as e:
            print(f"è­¦å‘Šï¼šç„¡æ³•è™•ç† {scan_file}: {e}")
            continue

    return strong_references, weak_references, layer_references

def scan_internal_references(docs):
    """æƒæ docs-ai æ–‡æª”ä¹‹é–“çš„äº’ç›¸å¼•ç”¨ï¼ˆå»ºç«‹æ–‡æª”ç¶²çµ¡ï¼‰"""
    internal_refs = defaultdict(lambda: {"strong": [], "weak": []})

    for doc_path, doc_info in docs.items():
        doc_file = DOCS_AI_PATH / doc_path

        try:
            with open(doc_file, 'r', encoding='utf-8') as f:
                content = f.read()

                # æƒæè©²æ–‡æª”å¼•ç”¨çš„å…¶ä»–æ–‡æª”ï¼ˆå¼·å¼•ç”¨ï¼‰
                strong_pattern = r'@docs-ai/([^\s\]]+\.md)'
                strong_matches = re.findall(strong_pattern, content)

                for match in strong_matches:
                    if match in docs and match != doc_path:
                        # è¨˜éŒ„ï¼šdoc_path å¼•ç”¨äº† matchï¼ˆå¼·å¼•ç”¨ï¼‰
                        doc_info["references_to"].append({"doc": match, "type": "strong"})
                        internal_refs[match]["strong"].append(doc_path)

                # æƒæè©²æ–‡æª”å¼•ç”¨çš„å…¶ä»–æ–‡æª”ï¼ˆå¼±å¼•ç”¨ï¼‰
                weak_pattern = r'(?<!@)docs-ai/([^\s\]]+\.md)'
                weak_matches = re.findall(weak_pattern, content)

                for match in weak_matches:
                    if match in docs and match != doc_path:
                        # è¨˜éŒ„ï¼šdoc_path å¼•ç”¨äº† matchï¼ˆå¼±å¼•ç”¨ï¼‰
                        doc_info["references_to"].append({"doc": match, "type": "weak"})
                        internal_refs[match]["weak"].append(doc_path)

        except Exception as e:
            print(f"è­¦å‘Šï¼šç„¡æ³•æƒæå…§éƒ¨å¼•ç”¨ {doc_file}: {e}")
            continue

    return internal_refs

def calculate_weighted_score(strong_refs, weak_refs):
    """è¨ˆç®—åŠ æ¬Šå¼•ç”¨åˆ†æ•¸"""
    return strong_refs * STRONG_WEIGHT + weak_refs * WEAK_WEIGHT

def categorize_importance_v2(strong_refs, weak_refs):
    """åŸºæ–¼åŠ æ¬Šåˆ†æ•¸çš„é‡è¦æ€§åˆ†é¡"""
    score = calculate_weighted_score(strong_refs, weak_refs)

    if score >= 10:
        return "critical"
    elif score >= 5:
        return "important"
    elif score >= 2:
        return "common"
    elif score > 0:
        return "referenced"
    else:
        return "unreferenced"

def generate_index():
    """ç”Ÿæˆå®Œæ•´çš„ç´¢å¼•æ–‡ä»¶"""
    print("ğŸ” æƒæ docs-ai æ–‡ä»¶...")
    docs = scan_docs_ai_files()

    print("ğŸ“Š æƒæ CLAUDE.md å¼•ç”¨...")
    strong_references, weak_references, layer_references = scan_claude_references()

    print("ğŸ•¸ï¸ æƒææ–‡æª”å…§éƒ¨å¼•ç”¨...")
    internal_refs = scan_internal_references(docs)

    # æ›´æ–°å¼•ç”¨è¨ˆæ•¸ï¼ˆä¾†è‡ª CLAUDE.md å’Œå…¶ä»–å¤–éƒ¨æª”æ¡ˆï¼‰
    for doc_path, ref_list in strong_references.items():
        if doc_path in docs:
            docs[doc_path]["strong_references"] = len(ref_list)
            docs[doc_path]["strong_referenced_by"] = ref_list

    for doc_path, ref_list in weak_references.items():
        if doc_path in docs:
            docs[doc_path]["weak_references"] = len(ref_list)
            docs[doc_path]["weak_referenced_by"] = ref_list

    # æ›´æ–°å±¤ç´šå¼•ç”¨ä¿¡æ¯
    for doc_path, layer_refs in layer_references.items():
        if doc_path in docs:
            docs[doc_path]["layer_references"] = dict(layer_refs)

    # æ›´æ–°å…§éƒ¨å¼•ç”¨è¨ˆæ•¸ï¼ˆä¾†è‡ªå…¶ä»– docs-ai æ–‡æª”ï¼‰
    for doc_path, refs in internal_refs.items():
        if doc_path in docs:
            # åŠ å…¥å…§éƒ¨å¼·å¼•ç”¨
            docs[doc_path]["strong_references"] += len(refs["strong"])
            docs[doc_path]["strong_referenced_by"].extend(
                [f"docs-ai/{ref}" for ref in refs["strong"]]
            )

            # åŠ å…¥å…§éƒ¨å¼±å¼•ç”¨
            docs[doc_path]["weak_references"] += len(refs["weak"])
            docs[doc_path]["weak_referenced_by"].extend(
                [f"docs-ai/{ref}" for ref in refs["weak"]]
            )

    # è¨ˆç®—åŠ æ¬Šåˆ†æ•¸å’Œé‡è¦æ€§ï¼Œä¸¦è¨­ç½®å‘å¾Œå…¼å®¹æ¬„ä½
    for doc_path, doc_info in docs.items():
        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        doc_info["total_weighted_refs"] = calculate_weighted_score(
            doc_info["strong_references"],
            doc_info["weak_references"]
        )

        # è¨­ç½®é‡è¦æ€§ç­‰ç´š
        doc_info["importance"] = categorize_importance_v2(
            doc_info["strong_references"],
            doc_info["weak_references"]
        )

        # å‘å¾Œå…¼å®¹ï¼šç¸½å¼•ç”¨æ¬¡æ•¸
        doc_info["references"] = doc_info["strong_references"] + doc_info["weak_references"]

        # å‘å¾Œå…¼å®¹ï¼šæ‰€æœ‰å¼•ç”¨ä¾†æº
        doc_info["referenced_by"] = list(set(
            doc_info["strong_referenced_by"] + doc_info["weak_referenced_by"]
        ))

    # çµ±è¨ˆä¿¡æ¯ï¼ˆå¢å¼·ç‰ˆï¼‰
    stats = {
        "total_docs": len(docs),

        # å¼•ç”¨çµ±è¨ˆ
        "total_strong_refs": sum(d["strong_references"] for d in docs.values()),
        "total_weak_refs": sum(d["weak_references"] for d in docs.values()),
        "total_references": sum(d["references"] for d in docs.values()),

        # å¹³å‡å¼•ç”¨
        "avg_strong_refs": round(
            sum(d["strong_references"] for d in docs.values()) / len(docs) if docs else 0,
            2
        ),
        "avg_weak_refs": round(
            sum(d["weak_references"] for d in docs.values()) / len(docs) if docs else 0,
            2
        ),

        # æ–‡æª”åˆ†é¡çµ±è¨ˆ
        "critical_docs": len([d for d in docs.values() if d["importance"] == "critical"]),
        "important_docs": len([d for d in docs.values() if d["importance"] == "important"]),
        "common_docs": len([d for d in docs.values() if d["importance"] == "common"]),
        "weakly_referenced_docs": len([
            d for d in docs.values()
            if d["weak_references"] > 0 and d["strong_references"] == 0
        ]),
        "unreferenced_docs": len([d for d in docs.values() if d["importance"] == "unreferenced"]),

        # ä¸‰å±¤æ¶æ§‹çµ±è¨ˆï¼ˆæ–°å¢ï¼‰
        "layer_distribution": {
            "layer1": {
                "name": LAYER_DEFINITIONS["layer1"]["name"],
                "description": LAYER_DEFINITIONS["layer1"]["description"],
                "count": len([d for d in docs.values() if d.get("layer") == "layer1"]),
                "docs": [d["path"] for d in docs.values() if d.get("layer") == "layer1"][:5]
            },
            "layer2": {
                "name": LAYER_DEFINITIONS["layer2"]["name"],
                "description": LAYER_DEFINITIONS["layer2"]["description"],
                "count": len([d for d in docs.values() if d.get("layer") == "layer2"]),
                "docs": [d["path"] for d in docs.values() if d.get("layer") == "layer2"][:5]
            },
            "layer3": {
                "name": LAYER_DEFINITIONS["layer3"]["name"],
                "description": LAYER_DEFINITIONS["layer3"]["description"],
                "count": len([d for d in docs.values() if d.get("layer") == "layer3"]),
                "docs": [d["path"] for d in docs.values() if d.get("layer") == "layer3"][:5]
            }
        },

        # å…§éƒ¨å¼•ç”¨ç¶²çµ¡çµ±è¨ˆ
        "internal_network_edges": sum(len(d["references_to"]) for d in docs.values()),

        # åˆ†é¡çµ±è¨ˆ
        "categories": {
            cat_key: {
                "name": cat_info["name"],
                "description": cat_info["description"],
                "count": len([d for d in docs.values() if d["category"] == cat_key])
            }
            for cat_key, cat_info in CATEGORIES.items()
        },

        "generated_at": datetime.now().isoformat(),
        "generated_by": "generate-docs-ai-index.py v3.0"  # å‡ç´šç‰ˆæœ¬è™Ÿ
    }

    # æ§‹å»ºæœ€çµ‚è¼¸å‡º
    output = {
        "version": "3.0",  # ç‰ˆæœ¬æ¨™è¨˜ï¼ˆå‡ç´šä»¥æ”¯æ´ä¸‰å±¤æ¶æ§‹ï¼‰
        "stats": stats,
        "categories": CATEGORIES,
        "layer_definitions": LAYER_DEFINITIONS,  # æ–°å¢ï¼šä¸‰å±¤æ¶æ§‹å®šç¾©
        "documents": docs,

        # æ’è¡Œæ¦œï¼ˆåŸºæ–¼åŠ æ¬Šåˆ†æ•¸ï¼‰
        "top_referenced": sorted(
            [d for d in docs.values() if d["total_weighted_refs"] > 0],
            key=lambda x: x["total_weighted_refs"],
            reverse=True
        )[:10],

        # åªæœ‰å¼·å¼•ç”¨çš„æ–‡æª”
        "strong_only": sorted(
            [d for d in docs.values() if d["strong_references"] > 0 and d["weak_references"] == 0],
            key=lambda x: x["strong_references"],
            reverse=True
        )[:5],

        # åªæœ‰å¼±å¼•ç”¨çš„æ–‡æª”
        "weak_only": sorted(
            [d for d in docs.values() if d["weak_references"] > 0 and d["strong_references"] == 0],
            key=lambda x: x["weak_references"],
            reverse=True
        )[:5],

        # ä¸‰å±¤æ¶æ§‹å°ˆç”¨æ’è¡Œæ¦œï¼ˆæ–°å¢ï¼‰
        "layer_top_docs": {
            "layer1": sorted(
                [d for d in docs.values() if d.get("layer") == "layer1"],
                key=lambda x: x["total_weighted_refs"],
                reverse=True
            )[:5],
            "layer2": sorted(
                [d for d in docs.values() if d.get("layer") == "layer2"],
                key=lambda x: x["total_weighted_refs"],
                reverse=True
            )[:5],
            "layer3": sorted(
                [d for d in docs.values() if d.get("layer") == "layer3"],
                key=lambda x: x["total_weighted_refs"],
                reverse=True
            )[:5]
        }
    }

    return output

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ“š RosAGV docs-ai ç´¢å¼•ç”Ÿæˆå™¨ v3.0")
    print("   æ”¯æ´å¼·å¼•ç”¨èˆ‡å¼±å¼•ç”¨å€åˆ†")
    print("   æ”¯æ´ä¸‰å±¤æ–‡æª”æ¶æ§‹åˆ†æ")
    print("=" * 60)

    # ç”Ÿæˆç´¢å¼•
    index = generate_index()

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    # å¯«å…¥ JSON æ–‡ä»¶
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ç´¢å¼•ç”ŸæˆæˆåŠŸï¼")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_PATH}")
    print(f"\nğŸ“Š çµ±è¨ˆä¿¡æ¯:")
    print(f"  - ç¸½æ–‡æª”æ•¸: {index['stats']['total_docs']}")
    print(f"  - å¼·å¼•ç”¨ç¸½æ•¸: {index['stats']['total_strong_refs']}")
    print(f"  - å¼±å¼•ç”¨ç¸½æ•¸: {index['stats']['total_weak_refs']}")
    print(f"  - ç¸½å¼•ç”¨æ•¸: {index['stats']['total_references']}")
    print(f"  - å¹³å‡å¼·å¼•ç”¨: {index['stats']['avg_strong_refs']}")
    print(f"  - å¹³å‡å¼±å¼•ç”¨: {index['stats']['avg_weak_refs']}")
    print(f"  - é—œéµæ–‡æª”: {index['stats']['critical_docs']}")
    print(f"  - é‡è¦æ–‡æª”: {index['stats']['important_docs']}")
    print(f"  - åªæœ‰å¼±å¼•ç”¨æ–‡æª”: {index['stats']['weakly_referenced_docs']}")
    print(f"  - æœªå¼•ç”¨æ–‡æª”: {index['stats']['unreferenced_docs']}")
    print(f"  - å…§éƒ¨å¼•ç”¨é‚Šæ•¸: {index['stats']['internal_network_edges']}")

    print(f"\nğŸ“š ä¸‰å±¤æ¶æ§‹åˆ†ä½ˆ:")
    for layer_key, layer_info in index['stats']['layer_distribution'].items():
        print(f"  {layer_info['name']}: {layer_info['count']} å€‹æ–‡æª”")
        if layer_info.get('docs'):
            for doc_path in layer_info['docs'][:3]:
                print(f"    - {doc_path}")

    print(f"\nğŸ”¥ æœ€å¸¸è¢«å¼•ç”¨çš„æ–‡æª”ï¼ˆåŸºæ–¼åŠ æ¬Šåˆ†æ•¸ï¼‰:")
    for i, doc in enumerate(index["top_referenced"][:5], 1):
        layer_name = LAYER_DEFINITIONS.get(doc.get('layer', 'layer2'), {}).get('name', 'æœªåˆ†å±¤')
        print(f"  {i}. {doc['title']} [{layer_name}]")
        print(f"     è·¯å¾‘: {doc['path']}")
        print(f"     å¼·å¼•ç”¨: {doc['strong_references']} | å¼±å¼•ç”¨: {doc['weak_references']}")
        print(f"     åŠ æ¬Šåˆ†æ•¸: {doc['total_weighted_refs']:.1f}")

    # é¡¯ç¤ºåªæœ‰å¼±å¼•ç”¨çš„æ–‡æª”ï¼ˆé€™äº›å¯èƒ½éœ€è¦æå‡ç‚ºå¼·å¼•ç”¨ï¼‰
    if index.get("weak_only"):
        print(f"\nâš ï¸ åªæœ‰å¼±å¼•ç”¨çš„æ–‡æª”ï¼ˆå¯èƒ½éœ€è¦æå‡ç‚ºå¼·å¼•ç”¨ï¼‰:")
        for doc in index["weak_only"][:3]:
            print(f"  - {doc['title']} ({doc['weak_references']} æ¬¡å¼±å¼•ç”¨)")

if __name__ == "__main__":
    main()