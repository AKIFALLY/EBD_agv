#!/usr/bin/env python3
"""
KUKA æ•´åˆæ¸¬è©¦é‹è¡Œè…³æœ¬
åŸ·è¡Œå®Œæ•´çš„ KUKA æ•´åˆæ¸¬è©¦å¥—ä»¶ï¼ŒåŒ…æ‹¬å–®å…ƒæ¸¬è©¦å’Œæ•´åˆæ¸¬è©¦
"""
import os
import sys
import subprocess
import argparse
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))
from mock_environment import MockTestEnvironment


class IntegrationTestRunner:
    """æ•´åˆæ¸¬è©¦é‹è¡Œå™¨"""
    
    def __init__(self):
        self.test_dir = Path(__file__).parent
        self.results = {
            "start_time": None,
            "end_time": None,
            "duration": 0,
            "tests": {
                "unit": {"passed": 0, "failed": 0, "skipped": 0},
                "integration": {"passed": 0, "failed": 0, "skipped": 0}
            },
            "coverage": {},
            "errors": []
        }
        self.mock_env = MockTestEnvironment()
    
    def setup_test_environment(self):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        print("ğŸ”§ è¨­ç½®æ¸¬è©¦ç’°å¢ƒ...")
        
        # å‰µå»ºæ¸¬è©¦å ´æ™¯
        self._create_test_scenarios()
        
        # è¨­ç½®ç’°å¢ƒè®Šæ•¸
        os.environ['PYTEST_CURRENT_TEST'] = 'true'
        os.environ['RCS_TEST_MODE'] = '1'
        
        print("âœ… æ¸¬è©¦ç’°å¢ƒè¨­ç½®å®Œæˆ")
    
    def _create_test_scenarios(self):
        """å‰µå»ºæ¸¬è©¦å ´æ™¯"""
        
        # å ´æ™¯1: åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
        basic_scenario = {
            "name": "basic_functionality",
            "description": "åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å ´æ™¯",
            "agvs": [
                {
                    "id": 101,
                    "name": "KUKA101",
                    "model": "KUKA400i",
                    "enable": 1,
                    "status_id": 3,  # idle
                    "x": 100.0,
                    "y": 200.0,
                    "heading": 90.0,
                    "battery": 85
                }
            ],
            "tasks": [
                {
                    "id": 1,
                    "name": "æ¸¬è©¦ç§»å‹•ä»»å‹™",
                    "description": "åŸºæœ¬ç§»å‹•ä»»å‹™æ¸¬è©¦",
                    "work_id": 210001,
                    "status_id": 1,  # pending
                    "priority": 50,
                    "room_id": None,
                    "agv_id": None,
                    "mission_code": None,
                    "parameters": {
                        "model": "KUKA400i",
                        "nodes": [10, 20, 30]
                    }
                }
            ],
            "racks": [
                {
                    "id": 1,
                    "name": "RACK001",
                    "agv_id": None,
                    "location_id": 10,
                    "product_id": None,
                    "is_carry": 0,
                    "is_in_map": 1,
                    "is_docked": 0,
                    "status_id": 1,
                    "direction": 0
                }
            ]
        }
        
        self.mock_env.add_test_scenario("basic", basic_scenario)
        
        # å ´æ™¯2: é«˜è² è¼‰æ¸¬è©¦
        high_load_scenario = {
            "name": "high_load",
            "description": "é«˜è² è¼‰æ¸¬è©¦å ´æ™¯",
            "agvs": [
                {
                    "id": 101 + i,
                    "name": f"KUKA{101 + i}",
                    "model": "KUKA400i",
                    "enable": 1,
                    "status_id": 3,
                    "x": 100.0 + i * 10,
                    "y": 200.0 + i * 10,
                    "heading": 90.0,
                    "battery": 80 - i * 5
                }
                for i in range(5)
            ],
            "tasks": [
                {
                    "id": i + 1,
                    "name": f"é«˜è² è¼‰ä»»å‹™{i + 1}",
                    "description": f"é«˜è² è¼‰æ¸¬è©¦ä»»å‹™{i + 1}",
                    "work_id": 220001,
                    "status_id": 1,
                    "priority": 50 + i,
                    "room_id": None,
                    "agv_id": None,
                    "mission_code": None,
                    "parameters": {
                        "model": "KUKA400i",
                        "nodes": [10 + i, 20 + i, 30 + i]
                    }
                }
                for i in range(10)
            ]
        }
        
        self.mock_env.add_test_scenario("high_load", high_load_scenario)
    
    def run_unit_tests(self, verbose: bool = False, coverage: bool = False) -> bool:
        """é‹è¡Œå–®å…ƒæ¸¬è©¦"""
        print("\nğŸ§ª é‹è¡Œå–®å…ƒæ¸¬è©¦...")
        
        cmd = ["python", "-m", "pytest"]
        
        if verbose:
            cmd.append("-v")
        
        if coverage:
            cmd.extend([
                "--cov=rcs.kuka_manager",
                "--cov=rcs.kuka_robot",
                "--cov=rcs.kuka_container",
                "--cov-report=json:coverage_unit.json"
            ])
        
        # æ·»åŠ å–®å…ƒæ¸¬è©¦æª”æ¡ˆ
        unit_test_files = [
            "test_kuka_robot.py",
            "test_kuka_container.py",
            "test_kuka_manager.py"
        ]
        
        for test_file in unit_test_files:
            cmd.append(str(self.test_dir / test_file))
        
        try:
            result = subprocess.run(
                cmd, 
                cwd=str(self.test_dir), 
                capture_output=True, 
                text=True,
                timeout=300  # 5 åˆ†é˜è¶…æ™‚
            )
            
            if result.returncode == 0:
                print("âœ… å–®å…ƒæ¸¬è©¦é€šé")
                self._parse_test_results(result.stdout, "unit")
                return True
            else:
                print("âŒ å–®å…ƒæ¸¬è©¦å¤±æ•—")
                print(result.stdout)
                print(result.stderr)
                self.results["errors"].append({
                    "type": "unit_test_failure",
                    "message": result.stderr
                })
                return False
                
        except subprocess.TimeoutExpired:
            print("âŒ å–®å…ƒæ¸¬è©¦è¶…æ™‚")
            self.results["errors"].append({
                "type": "unit_test_timeout",
                "message": "Unit tests timed out after 5 minutes"
            })
            return False
        except Exception as e:
            print(f"âŒ å–®å…ƒæ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
            self.results["errors"].append({
                "type": "unit_test_error",
                "message": str(e)
            })
            return False
    
    def run_integration_tests(self, verbose: bool = False, coverage: bool = False) -> bool:
        """é‹è¡Œæ•´åˆæ¸¬è©¦"""
        print("\nğŸ”— é‹è¡Œæ•´åˆæ¸¬è©¦...")
        
        cmd = ["python", "-m", "pytest"]
        
        if verbose:
            cmd.append("-v")
        
        if coverage:
            cmd.extend([
                "--cov=rcs.kuka_manager",
                "--cov=rcs.kuka_robot",
                "--cov=rcs.kuka_container",
                "--cov-append",
                "--cov-report=json:coverage_integration.json"
            ])
        
        # æ·»åŠ æ•´åˆæ¸¬è©¦æª”æ¡ˆ
        cmd.append(str(self.test_dir / "test_kuka_integration.py"))
        
        try:
            result = subprocess.run(
                cmd,
                cwd=str(self.test_dir),
                capture_output=True,
                text=True,
                timeout=600  # 10 åˆ†é˜è¶…æ™‚
            )
            
            if result.returncode == 0:
                print("âœ… æ•´åˆæ¸¬è©¦é€šé")
                self._parse_test_results(result.stdout, "integration")
                return True
            else:
                print("âŒ æ•´åˆæ¸¬è©¦å¤±æ•—")
                print(result.stdout)
                print(result.stderr)
                self.results["errors"].append({
                    "type": "integration_test_failure",
                    "message": result.stderr
                })
                return False
                
        except subprocess.TimeoutExpired:
            print("âŒ æ•´åˆæ¸¬è©¦è¶…æ™‚")
            self.results["errors"].append({
                "type": "integration_test_timeout",
                "message": "Integration tests timed out after 10 minutes"
            })
            return False
        except Exception as e:
            print(f"âŒ æ•´åˆæ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
            self.results["errors"].append({
                "type": "integration_test_error",
                "message": str(e)
            })
            return False
    
    def run_scenario_tests(self, scenario: str = "basic") -> bool:
        """é‹è¡Œå ´æ™¯æ¸¬è©¦"""
        print(f"\nğŸ­ é‹è¡Œå ´æ™¯æ¸¬è©¦: {scenario}")
        
        try:
            # è¼‰å…¥æ¸¬è©¦å ´æ™¯
            self.mock_env.load_scenario(scenario)
            
            # åŒ¯å‡ºå ´æ™¯ç‹€æ…‹ä¾›æ¸¬è©¦ä½¿ç”¨
            scenario_file = self.test_dir / f"scenario_{scenario}.json"
            self.mock_env.export_state(str(scenario_file))
            
            # é‹è¡Œå ´æ™¯ç›¸é—œæ¸¬è©¦
            cmd = [
                "python", "-m", "pytest", 
                "-v",
                "-k", f"test_{scenario}",
                str(self.test_dir / "test_kuka_integration.py")
            ]
            
            result = subprocess.run(
                cmd,
                cwd=str(self.test_dir),
                capture_output=True,
                text=True,
                timeout=300
            )
            
            # æ¸…ç†å ´æ™¯æª”æ¡ˆ
            if scenario_file.exists():
                scenario_file.unlink()
            
            if result.returncode == 0:
                print(f"âœ… å ´æ™¯æ¸¬è©¦ {scenario} é€šé")
                return True
            else:
                print(f"âŒ å ´æ™¯æ¸¬è©¦ {scenario} å¤±æ•—")
                print(result.stdout)
                print(result.stderr)
                return False
                
        except Exception as e:
            print(f"âŒ å ´æ™¯æ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
            return False
    
    def _parse_test_results(self, stdout: str, test_type: str):
        """è§£ææ¸¬è©¦çµæœ"""
        lines = stdout.split('\n')
        
        for line in lines:
            if "passed" in line and "failed" in line:
                # è§£æ pytest çµæœè¡Œï¼Œå¦‚: "5 passed, 1 failed, 2 skipped"
                parts = line.split(',')
                for part in parts:
                    part = part.strip()
                    if "passed" in part:
                        count = int(part.split()[0])
                        self.results["tests"][test_type]["passed"] = count
                    elif "failed" in part:
                        count = int(part.split()[0])
                        self.results["tests"][test_type]["failed"] = count
                    elif "skipped" in part:
                        count = int(part.split()[0])
                        self.results["tests"][test_type]["skipped"] = count
                break
    
    def generate_report(self, output_file: str = None):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
        
        self.results["end_time"] = datetime.now().isoformat()
        
        if self.results["start_time"]:
            start = datetime.fromisoformat(self.results["start_time"])
            end = datetime.fromisoformat(self.results["end_time"])
            self.results["duration"] = (end - start).total_seconds()
        
        # è¨ˆç®—ç¸½è¨ˆ
        total_passed = (self.results["tests"]["unit"]["passed"] + 
                       self.results["tests"]["integration"]["passed"])
        total_failed = (self.results["tests"]["unit"]["failed"] + 
                       self.results["tests"]["integration"]["failed"])
        total_skipped = (self.results["tests"]["unit"]["skipped"] + 
                        self.results["tests"]["integration"]["skipped"])
        
        # ç”Ÿæˆå ±å‘Š
        report = f"""
# KUKA æ•´åˆæ¸¬è©¦å ±å‘Š

## æ¸¬è©¦æ¦‚è¦
- **æ¸¬è©¦æ™‚é–“**: {self.results["start_time"]} - {self.results["end_time"]}
- **æ¸¬è©¦æŒçºŒæ™‚é–“**: {self.results["duration"]:.2f} ç§’
- **ç¸½æ¸¬è©¦æ•¸**: {total_passed + total_failed + total_skipped}
- **é€šé**: {total_passed}
- **å¤±æ•—**: {total_failed}
- **è·³é**: {total_skipped}
- **æˆåŠŸç‡**: {(total_passed / max(total_passed + total_failed, 1) * 100):.1f}%

## å–®å…ƒæ¸¬è©¦çµæœ
- **é€šé**: {self.results["tests"]["unit"]["passed"]}
- **å¤±æ•—**: {self.results["tests"]["unit"]["failed"]}
- **è·³é**: {self.results["tests"]["unit"]["skipped"]}

## æ•´åˆæ¸¬è©¦çµæœ
- **é€šé**: {self.results["tests"]["integration"]["passed"]}
- **å¤±æ•—**: {self.results["tests"]["integration"]["failed"]}
- **è·³é**: {self.results["tests"]["integration"]["skipped"]}

## éŒ¯èª¤è¨˜éŒ„
"""
        
        if self.results["errors"]:
            for error in self.results["errors"]:
                report += f"- **{error['type']}**: {error['message']}\n"
        else:
            report += "ç„¡éŒ¯èª¤è¨˜éŒ„\n"
        
        # è¼¸å‡ºå ±å‘Š
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“ å ±å‘Šå·²å„²å­˜è‡³: {output_file}")
        else:
            print(report)
        
        # åŒæ™‚ç”Ÿæˆ JSON æ ¼å¼å ±å‘Š
        json_file = output_file.replace('.md', '.json') if output_file else 'test_results.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"ğŸ“Š JSON å ±å‘Šå·²å„²å­˜è‡³: {json_file}")
    
    def run_all_tests(self, verbose: bool = False, coverage: bool = False, scenarios: List[str] = None) -> bool:
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹ KUKA æ•´åˆæ¸¬è©¦å¥—ä»¶")
        print("=" * 60)
        
        self.results["start_time"] = datetime.now().isoformat()
        
        # è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        self.setup_test_environment()
        
        success = True
        
        # é‹è¡Œå–®å…ƒæ¸¬è©¦
        if not self.run_unit_tests(verbose, coverage):
            success = False
        
        # é‹è¡Œæ•´åˆæ¸¬è©¦
        if not self.run_integration_tests(verbose, coverage):
            success = False
        
        # é‹è¡Œå ´æ™¯æ¸¬è©¦
        if scenarios:
            for scenario in scenarios:
                if not self.run_scenario_tests(scenario):
                    success = False
        
        return success


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="é‹è¡Œ KUKA æ•´åˆæ¸¬è©¦å¥—ä»¶")
    parser.add_argument("-v", "--verbose", action="store_true", help="è©³ç´°è¼¸å‡º")
    parser.add_argument("--coverage", action="store_true", help="ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š")
    parser.add_argument("--unit-only", action="store_true", help="åªé‹è¡Œå–®å…ƒæ¸¬è©¦")
    parser.add_argument("--integration-only", action="store_true", help="åªé‹è¡Œæ•´åˆæ¸¬è©¦")
    parser.add_argument("--scenarios", nargs="*", help="é‹è¡ŒæŒ‡å®šå ´æ™¯æ¸¬è©¦", 
                       choices=["basic", "high_load"], default=["basic"])
    parser.add_argument("--output", help="æ¸¬è©¦å ±å‘Šè¼¸å‡ºæª”æ¡ˆ", default="kuka_test_report.md")
    
    args = parser.parse_args()
    
    runner = IntegrationTestRunner()
    
    try:
        if args.unit_only:
            success = runner.run_unit_tests(args.verbose, args.coverage)
        elif args.integration_only:
            success = runner.run_integration_tests(args.verbose, args.coverage)
        else:
            success = runner.run_all_tests(args.verbose, args.coverage, args.scenarios)
        
        # ç”Ÿæˆå ±å‘Š
        runner.generate_report(args.output)
        
        if success:
            print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
            sys.exit(0)
        else:
            print("\nâŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ä½¿ç”¨è€…ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()