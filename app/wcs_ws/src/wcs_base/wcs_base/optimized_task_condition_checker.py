"""
å„ªåŒ–çš„ä»»å‹™æ¢ä»¶æª¢æŸ¥å™¨
å¢å¼·äº†å¿«å–æ©Ÿåˆ¶ï¼Œé¿å…é‡è¤‡æŸ¥è©¢å·²ç²å–çš„è®Šæ•¸

ä¸»è¦å„ªåŒ–ï¼š
1. è®Šæ•¸å¿«å–æ©Ÿåˆ¶ - ä¿å­˜å·²æŸ¥è©¢çš„è®Šæ•¸é¿å…é‡è¤‡æŸ¥è©¢
2. æ¢ä»¶çµæœç¹¼æ‰¿ - å¾ŒçºŒæ¢ä»¶å¯ä»¥ä½¿ç”¨å‰é¢æ¢ä»¶çš„æŸ¥è©¢çµæœ
3. æ™ºèƒ½ SQL å„ªåŒ– - æª¢æ¸¬ä¸¦è·³éé‡è¤‡çš„è³‡æ–™åº«æŸ¥è©¢
4. ä½ç½®è¨ˆç®—å¿«å– - å¿«å–æˆ¿é–“è™Ÿã€ç›®æ¨™ä½ç½®ç­‰è¨ˆç®—çµæœ
"""

from db_proxy.crud.task_condition_crud import task_condition_crud
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError
import json
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Tuple
from rclpy.node import Node
from .task_condition_config import get_current_config, TaskConditionConfig
import re


class OptimizedTaskConditionChecker:
    """
    å„ªåŒ–çš„ä»»å‹™æ¢ä»¶æª¢æŸ¥å™¨
    
    æä¾›åŸºæ–¼ task_condition è¡¨æ ¼çš„é€šç”¨æ¢ä»¶æª¢æŸ¥åŠŸèƒ½ï¼Œ
    æ”¯æ´è®Šæ•¸å¿«å–å’Œæ¢ä»¶çµæœç¹¼æ‰¿ä»¥é¿å…é‡è¤‡æŸ¥è©¢ã€‚
    """

    def __init__(self, db_manager, logger, config: Optional[TaskConditionConfig] = None, **kwargs):
        """
        åˆå§‹åŒ–å„ªåŒ–æ¢ä»¶æª¢æŸ¥å™¨

        Args:
            db_manager: è³‡æ–™åº«ç®¡ç†å™¨
            logger: æ—¥èªŒè¨˜éŒ„å™¨
            config: ä»»å‹™æ¢ä»¶é…ç½®ç‰©ä»¶
            **kwargs: é¡å¤–çš„é…ç½®åƒæ•¸
        """
        self.db_manager = db_manager
        self.logger = logger

        # è¼‰å…¥é…ç½®
        if config is None:
            config = get_current_config()

        # æ‡‰ç”¨é…ç½®
        self.real_time_mode = kwargs.get('real_time_mode', config.real_time_mode)
        self.query_timeout = kwargs.get('query_timeout', config.query_timeout)
        self.max_iterations = kwargs.get('max_iterations', config.max_iterations)
        self.enable_sql_validation = kwargs.get('enable_sql_validation', config.enable_sql_validation)
        self.log_sql_queries = kwargs.get('log_sql_queries', config.log_sql_queries)

        # === å¿«å–æ©Ÿåˆ¶ ===
        self.variable_cache = {}          # è®Šæ•¸å¿«å–ï¼šä¿å­˜å·²æŸ¥è©¢çš„è®Šæ•¸
        self.query_result_cache = {}      # æŸ¥è©¢çµæœå¿«å–ï¼šä¿å­˜ SQL æŸ¥è©¢çµæœ
        self.calculation_cache = {}       # è¨ˆç®—çµæœå¿«å–ï¼šä¿å­˜ä½ç½®ã€æˆ¿é–“è™Ÿç­‰è®¡ç®—
        self.collected_data = {}          # æ”¶é›†çš„æ¢ä»¶è³‡æ–™
        
        # === List ç‹€æ…‹è¨˜æ†¶ ===
        self.current_list = None
        self.current_list_index = 0
        self.list_stack = []
        self.processed_lists = set()

        mode_name = "å³æ™‚æŸ¥è©¢(å¿«å–å„ªåŒ–)" if self.real_time_mode else "é å­˜çµæœ(å¿«å–å„ªåŒ–)"
        self.logger.info(f"ğŸš€ OptimizedTaskConditionChecker åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å¼: {mode_name}")
        self.logger.debug(f"   å¿«å–åŠŸèƒ½å·²å•Ÿç”¨ï¼šè®Šæ•¸å¿«å–ã€æŸ¥è©¢å¿«å–ã€è¨ˆç®—å¿«å–")

    def check_conditions_from_id(self, start_id: int = 1) -> Tuple[bool, Dict[str, Any]]:
        """
        å¾æŒ‡å®š ID é–‹å§‹é€²è¡Œæ¢ä»¶æª¢æŸ¥ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            start_id: èµ·å§‹æ¢ä»¶ ID
            
        Returns:
            Tuple[bool, Dict[str, Any]]: (æ˜¯å¦æˆåŠŸ, æ”¶é›†çš„è³‡æ–™)
        """
        try:
            # é‡ç½®å¿«å–ï¼ˆæ–°çš„æª¢æŸ¥æœƒè©±ï¼‰
            self.reset_caches()
            current_id = start_id
            iteration_count = 0

            self.logger.info(f"ğŸ” é–‹å§‹å„ªåŒ–æ¢ä»¶æª¢æŸ¥ï¼Œèµ·å§‹ ID: {current_id}")
            
            while iteration_count < self.max_iterations:
                iteration_count += 1
                
                # å–å¾—ç•¶å‰ ID çš„æ¢ä»¶è³‡æ–™
                condition_result = self.get_task_condition_results(current_id)
                if not condition_result:
                    self.logger.warning(f"âš ï¸ ç„¡æ³•å–å¾— ID {current_id} çš„æ¢ä»¶è³‡æ–™")
                    current_id = start_id
                    continue
                
                # å„ªåŒ–è§£æï¼šæª¢æŸ¥å¿«å–ä¸¦é¿å…é‡è¤‡æŸ¥è©¢
                parse_result = self.parse_condition_results_optimized(current_id, condition_result)
                if not parse_result:
                    self.logger.warning(f"âš ï¸ ID {current_id} å„ªåŒ–è§£æå¤±æ•—")
                    current_id = start_id
                    continue
                
                success, data_list = parse_result
                
                if not success:
                    self.logger.info(f"ğŸ“‹ ID {current_id} æŸ¥è©¢æœªæˆåŠŸï¼Œå›åˆ°èµ·å§‹é»")
                    current_id = start_id
                    continue
                
                # è™•ç†è³‡æ–™åˆ—è¡¨
                next_id = None
                for data_item in data_list:
                    # æª¢æŸ¥çµæŸæ¢ä»¶
                    end_value = data_item.get("end")
                    if (end_value is True or end_value == "True" or 
                        end_value == "true" or end_value == 1):
                        # æ”¶é›†è³‡æ–™ä¸¦è¿”å›
                        self.collect_data_optimized(current_id, data_item)
                        self.logger.info(f"âœ… é‡åˆ°çµæŸæ¢ä»¶ï¼Œå„ªåŒ–æ¢ä»¶æª¢æŸ¥å®Œæˆ")
                        self.logger.info(f"ğŸ“Š å¿«å–çµ±è¨ˆ: è®Šæ•¸ {len(self.variable_cache)}, æŸ¥è©¢ {len(self.query_result_cache)}, è¨ˆç®— {len(self.calculation_cache)}")
                        return True, self.collected_data
                    
                    # æª¢æŸ¥ result æ¬„ä½
                    result_value = data_item.get("result")
                    if result_value == "True":
                        # æ”¶é›†è³‡æ–™
                        self.collect_data_optimized(current_id, data_item)

                        # å–å¾— next_id
                        next_id_raw = data_item.get("next_id")
                        if next_id_raw:
                            next_id = str(next_id_raw)
                            self.logger.info(f"ğŸ“ ID {current_id} æ¢ä»¶æ»¿è¶³ï¼Œnext_id: {next_id}")

                            if next_id.lower() == "end":
                                self.logger.info(f"âœ… é‡åˆ°çµæŸæ¨™è¨˜ï¼Œå„ªåŒ–æ¢ä»¶æª¢æŸ¥å®Œæˆ")
                                return True, self.collected_data
                            break
                    elif result_value == "False":
                        # æ”¯æ´ OR é‚è¼¯
                        next_id_raw = data_item.get("next_id")
                        if next_id_raw:
                            next_id = str(next_id_raw)
                            self.logger.info(f"ğŸ“‹ ID {current_id} æ¢ä»¶ä¸æ»¿è¶³ï¼Œæ¢ç´¢ next_id: {next_id}")
                            if next_id.lower() == "end":
                                return False, self.collected_data
                            break
                        else:
                            continue
                
                # è™•ç† next_id
                if next_id:
                    next_id_result = self.process_next_id(next_id)
                    if next_id_result == -1:
                        self.logger.info(f"ğŸ“‹ List éæ­·å®Œæˆï¼Œç„¡æ»¿è¶³æ¢ä»¶ï¼ŒçµæŸæª¢æŸ¥")
                        return False, self.collected_data
                    elif next_id_result is not None:
                        current_id = next_id_result
                    else:
                        current_id = start_id
                else:
                    # æ²’æœ‰ next_id çš„è™•ç†é‚è¼¯
                    if data_list:
                        all_false_no_next = all(
                            item.get("result") == "False" and not item.get("next_id")
                            for item in data_list
                        )
                        if all_false_no_next:
                            backtrack_id = self._try_backtrack_to_list()
                            if backtrack_id is not None:
                                current_id = backtrack_id
                                continue
                            else:
                                return False, self.collected_data
                    current_id = start_id
            
            # é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸
            self.logger.warning(f"âš ï¸ é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ {self.max_iterations}ï¼Œåœæ­¢å„ªåŒ–æ¢ä»¶æª¢æŸ¥")
            return False, self.collected_data
            
        except Exception as e:
            self.logger.error(f"âŒ å„ªåŒ–æ¢ä»¶æª¢æŸ¥å¤±æ•—: {e}")
            return False, {}

    def parse_condition_results_optimized(self, condition_id: int, condition_data: Dict[str, Any]) -> Optional[Tuple[bool, List[Dict[str, Any]]]]:
        """
        å„ªåŒ–çš„æ¢ä»¶çµæœè§£æ
        
        æª¢æŸ¥å¿«å–ï¼Œé¿å…é‡è¤‡æŸ¥è©¢ï¼Œä¸¦åˆ©ç”¨å·²æœ‰è®Šæ•¸é€²è¡Œè¨ˆç®—
        """
        try:
            conditions_sql = condition_data.get("conditions", "")
            
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥å¾å¿«å–ä¸­ç²å–çµæœ
            cache_key = f"condition_{condition_id}_{hash(conditions_sql)}"
            if cache_key in self.query_result_cache:
                self.logger.debug(f"ğŸ¯ æ¢ä»¶ {condition_id} å‘½ä¸­æŸ¥è©¢å¿«å–")
                return self.query_result_cache[cache_key]
            
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨è®Šæ•¸å¿«å–å„ªåŒ– SQL
            optimized_sql = self.optimize_sql_with_cache(conditions_sql, condition_id)
            
            if not self.real_time_mode:
                # é å­˜çµæœæ¨¡å¼
                results_data = condition_data.get("results", {})
                if results_data:
                    result = (True, [results_data])
                    self.query_result_cache[cache_key] = result
                    return result
            
            # å³æ™‚æŸ¥è©¢æ¨¡å¼ï¼ˆä½¿ç”¨å„ªåŒ–çš„ SQLï¼‰
            if optimized_sql:
                with self.db_manager.get_session() as session:
                    try:
                        if self.log_sql_queries:
                            self.logger.debug(f"ğŸ” åŸ·è¡Œå„ªåŒ– SQL (æ¢ä»¶ {condition_id}): {optimized_sql[:200]}...")
                        
                        query_result = session.execute(text(optimized_sql))
                        rows = query_result.fetchall()
                        
                        if rows:
                            data_list = []
                            for row in rows:
                                row_dict = dict(row._mapping)
                                # å°‡çµæœå­˜å…¥è®Šæ•¸å¿«å–
                                self.update_variable_cache(row_dict)
                                data_list.append(row_dict)
                            
                            result = (True, data_list)
                            self.query_result_cache[cache_key] = result
                            self.logger.debug(f"âœ… æ¢ä»¶ {condition_id} å„ªåŒ–æŸ¥è©¢æˆåŠŸï¼Œç²å¾— {len(data_list)} ç­†çµæœ")
                            return result
                        else:
                            result = (False, [])
                            self.query_result_cache[cache_key] = result
                            return result
                            
                    except SQLAlchemyError as e:
                        self.logger.error(f"âŒ æ¢ä»¶ {condition_id} å„ªåŒ– SQL åŸ·è¡Œå¤±æ•—: {e}")
                        return None
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ æ¢ä»¶ {condition_id} å„ªåŒ–è§£æå¤±æ•—: {e}")
            return None

    def optimize_sql_with_cache(self, sql: str, condition_id: int) -> str:
        """
        ä½¿ç”¨å¿«å–å„ªåŒ– SQL æŸ¥è©¢
        
        æª¢æŸ¥ SQL ä¸­éœ€è¦çš„è®Šæ•¸æ˜¯å¦å·²åœ¨å¿«å–ä¸­ï¼Œå¦‚æœæ˜¯å‰‡ç›´æ¥æ›¿æ›ç‚ºå¿«å–å€¼
        """
        try:
            optimized_sql = sql
            
            # æª¢æŸ¥å¸¸è¦‹çš„é‡è¤‡è¨ˆç®—æ¨¡å¼
            patterns = {
                # æˆ¿é–“è™Ÿè¨ˆç®—: location_id / 10000
                r'(\w+\.)?location_id\s*/\s*10000': lambda m: self._get_cached_room_id(),
                
                # ç›®æ¨™ä½ç½®è¨ˆç®—: room_id * 10000 + 1/2
                r'\(\s*(\w+\.)?location_id\s*/\s*10000\s*\)\s*\*\s*10000\s*\+\s*(\d+)': 
                    lambda m: self._get_cached_target_location(int(m.group(2))),
                    
                # ä¾†æºä½ç½®: MIN(location_id) å¦‚æœå·²å¿«å–
                r'MIN\((\w+\.)?location_id\)': lambda m: self._get_cached_source_location(),
                
                # æ–™æ¶ID: MIN(id) å¦‚æœå·²å¿«å–  
                r'MIN\((\w+\.)?id\)': lambda m: self._get_cached_rack_id(),
            }
            
            # æ‡‰ç”¨å„ªåŒ–æ¨¡å¼
            for pattern, replacement_func in patterns.items():
                if re.search(pattern, optimized_sql):
                    try:
                        cached_value = replacement_func(re.search(pattern, optimized_sql))
                        if cached_value is not None:
                            optimized_sql = re.sub(pattern, str(cached_value), optimized_sql)
                            self.logger.debug(f"ğŸ¯ æ¢ä»¶ {condition_id} ä½¿ç”¨å¿«å–å€¼æ›¿æ›: {pattern[:30]}... â†’ {cached_value}")
                    except Exception as e:
                        self.logger.debug(f"âš ï¸ å¿«å–æ›¿æ›å¤±æ•—: {e}")
                        continue
            
            return optimized_sql
            
        except Exception as e:
            self.logger.error(f"âŒ SQL å¿«å–å„ªåŒ–å¤±æ•—: {e}")
            return sql

    def _get_cached_room_id(self) -> Optional[int]:
        """å¾å¿«å–ä¸­ç²å–æˆ¿é–“è™Ÿ"""
        return self.variable_cache.get('room_id') or self.calculation_cache.get('room_id')
    
    def _get_cached_target_location(self, suffix: int) -> Optional[int]:
        """å¾å¿«å–ä¸­ç²å–ç›®æ¨™ä½ç½®"""
        cache_key = f'target_location_{suffix}'
        cached = self.calculation_cache.get(cache_key)
        if cached:
            return cached
            
        # å˜—è©¦è¨ˆç®—
        room_id = self._get_cached_room_id()
        if room_id:
            target_location = room_id * 10000 + suffix
            self.calculation_cache[cache_key] = target_location
            return target_location
        return None
    
    def _get_cached_source_location(self) -> Optional[int]:
        """å¾å¿«å–ä¸­ç²å–ä¾†æºä½ç½®"""
        return self.variable_cache.get('source_location')
    
    def _get_cached_rack_id(self) -> Optional[int]:
        """å¾å¿«å–ä¸­ç²å–æ–™æ¶ID"""
        return self.variable_cache.get('rack_id')

    def collect_data_optimized(self, condition_id: int, data_item: Dict[str, Any]):
        """
        å„ªåŒ–çš„è³‡æ–™æ”¶é›†
        
        æ”¶é›†è³‡æ–™ä¸¦æ›´æ–°è®Šæ•¸å¿«å–
        """
        # æ”¶é›†åˆ° collected_data
        for key, value in data_item.items():
            if key not in ["result", "next_id", "end"] and value is not None:
                self.collected_data[key] = value
        
        # æ¨™è¨˜æ¢ä»¶ ID
        self.collected_data["_condition_id"] = condition_id
        
        # æ›´æ–°è®Šæ•¸å¿«å–
        self.update_variable_cache(data_item)
        
        # è‡ªå‹•è¨ˆç®—å¸¸è¦‹çš„è¡ç”Ÿè®Šæ•¸
        self.calculate_derived_variables(data_item)
        
        self.logger.debug(f"ğŸ“Š æ¢ä»¶ {condition_id} æ”¶é›†è³‡æ–™: {len(data_item)} é …")

    def update_variable_cache(self, data_item: Dict[str, Any]):
        """
        æ›´æ–°è®Šæ•¸å¿«å–
        """
        for key, value in data_item.items():
            if key not in ["result", "next_id", "end"] and value is not None:
                self.variable_cache[key] = value

    def calculate_derived_variables(self, data_item: Dict[str, Any]):
        """
        è¨ˆç®—è¡ç”Ÿè®Šæ•¸ä¸¦å¿«å–
        """
        # å¾ source_location è¨ˆç®— room_id
        if 'source_location' in data_item and 'room_id' not in self.variable_cache:
            source_location = data_item['source_location']
            if isinstance(source_location, int) and source_location > 10000:
                room_id = source_location // 10000
                self.variable_cache['room_id'] = room_id
                self.calculation_cache['room_id'] = room_id
                self.logger.debug(f"ğŸ§® è‡ªå‹•è¨ˆç®— room_id: {source_location} // 10000 = {room_id}")
        
        # å¾ room_id è¨ˆç®—å¸¸è¦‹çš„ç›®æ¨™ä½ç½®
        room_id = self.variable_cache.get('room_id')
        if room_id:
            for suffix in [1, 2]:  # å…¥å£å’Œå‡ºå£
                cache_key = f'target_location_{suffix}'
                if cache_key not in self.calculation_cache:
                    target_location = room_id * 10000 + suffix
                    self.calculation_cache[cache_key] = target_location
                    self.logger.debug(f"ğŸ§® è‡ªå‹•è¨ˆç®— {cache_key}: {room_id} * 10000 + {suffix} = {target_location}")

    def reset_caches(self):
        """é‡ç½®æ‰€æœ‰å¿«å–"""
        self.variable_cache.clear()
        self.query_result_cache.clear()
        self.calculation_cache.clear()
        self.collected_data.clear()
        self.processed_lists.clear()
        self.logger.debug("ğŸ”„ å¿«å–å·²é‡ç½®")

    def get_task_condition_results(self, condition_id: int) -> Optional[Dict[str, Any]]:
        """
        å–å¾—ä»»å‹™æ¢ä»¶çµæœï¼ˆç»§æ‰¿åŸå§‹å¯¦ä½œï¼‰
        """
        try:
            with self.db_manager.get_session() as session:
                condition = task_condition_crud.get_by_id(session, condition_id)
                if condition:
                    return {
                        "id": condition.id,
                        "conditions": condition.conditions,
                        "results": condition.results or {},
                        "description": condition.description
                    }
                else:
                    self.logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°æ¢ä»¶ ID: {condition_id}")
                    return None
        except Exception as e:
            self.logger.error(f"âŒ å–å¾—æ¢ä»¶ {condition_id} å¤±æ•—: {e}")
            return None

    def process_next_id(self, next_id: str) -> Optional[int]:
        """
        è™•ç† next_idï¼ˆç»§æ‰¿åŸå§‹å¯¦ä½œï¼‰
        """
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚º List æ ¼å¼
            if next_id.startswith('[') and next_id.endswith(']'):
                # è§£æ List
                try:
                    id_list = json.loads(next_id)
                    if isinstance(id_list, list) and id_list:
                        # è¨­å®š List ç‹€æ…‹
                        self.current_list = id_list
                        self.current_list_index = 0
                        # è¿”å›ç¬¬ä¸€å€‹ ID
                        return id_list[0]
                except json.JSONDecodeError:
                    self.logger.error(f"âŒ ç„¡æ³•è§£æ next_id List: {next_id}")
                    return None
            else:
                # å–®ä¸€ ID
                try:
                    return int(next_id)
                except ValueError:
                    self.logger.error(f"âŒ ç„¡æ³•è½‰æ› next_id ç‚ºæ•´æ•¸: {next_id}")
                    return None
        except Exception as e:
            self.logger.error(f"âŒ è™•ç† next_id å¤±æ•—: {e}")
            return None

    def _try_backtrack_to_list(self) -> Optional[int]:
        """
        å˜—è©¦å›æº¯åˆ° List ç¹¼çºŒè™•ç†ï¼ˆç»§æ‰¿åŸå§‹å¯¦ä½œï¼‰
        """
        if self.current_list and self.current_list_index < len(self.current_list) - 1:
            self.current_list_index += 1
            return self.current_list[self.current_list_index]
        return None

    def get_cache_statistics(self) -> Dict[str, Any]:
        """
        å–å¾—å¿«å–çµ±è¨ˆè³‡è¨Š
        """
        return {
            'variable_cache_size': len(self.variable_cache),
            'query_result_cache_size': len(self.query_result_cache),
            'calculation_cache_size': len(self.calculation_cache),
            'collected_data_size': len(self.collected_data),
            'variable_cache_keys': list(self.variable_cache.keys()),
            'calculation_cache_keys': list(self.calculation_cache.keys())
        }